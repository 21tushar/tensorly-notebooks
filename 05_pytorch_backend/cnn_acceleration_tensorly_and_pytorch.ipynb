{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerating deep neural networks with tensor decompositions\n",
    "\n",
    "In this notebook we will show how to use TensorLy for accelerating convolutional layers in PyTorch.\n",
    "\n",
    "This notebook is based on this blog post: https://jacobgil.github.io/deeplearning/tensor-decompositions-deep-learning\n",
    "\n",
    "We will use the Tucker and CP decomositions to make low rank approximations of convolutional layers, and make them:\n",
    "1. Faster.\n",
    "2. Smaller.\n",
    "\n",
    "Like pruning, these are tools that can be used to make the networks smaller and faster, and then try to restore accuracy by training further.\n",
    "\n",
    "---\n",
    "\n",
    "First lets get VGG19 from torch vision: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d (3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (5): Conv2d (64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (10): Conv2d (128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace)\n",
      "    (18): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (19): Conv2d (256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace)\n",
      "    (25): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace)\n",
      "    (27): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (28): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace)\n",
      "    (32): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace)\n",
      "    (34): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace)\n",
      "    (36): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=4096)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=4096, out_features=1000)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "model = models.vgg19(pretrained=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has two parts. \n",
    "\n",
    "model.features is the convolutional part, containing only convolutional layers, ReLU and max pooling layers.\n",
    "\n",
    "\n",
    "model.classifier contains several fully connected and ReLU layers and does the final classification on features extracted from model.features. \n",
    "\n",
    "We will be focusing on model.features. Lets look at one of the convolutional layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "layer = model.features._modules['2']\n",
    "print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer performs 64 different convolutions (the output channels), \n",
    "\n",
    "where each convolution has a 3x3 spatial size and is done on an image with 64 channels (the input channels).\n",
    "\n",
    "If we call our convolutional kernel K, the input image X and the layer bias b, then the output from the layer, V will be:\n",
    "\n",
    "$$ V(x, y, t) = \\sum_i \\sum_j \\sum_sK(i, j, s, t)X(x-i, x-j, s) + b(t) $$ \n",
    "\n",
    "\n",
    "The weights of the layer are stored in a 4 dimensional tensor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(layer.weight.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use TensorLy for applying tensor decompositions on this 4 dimensional weight tensor.\n",
    "\n",
    "\n",
    "## CP Decomposition on convolutional layers\n",
    "\n",
    "The idea here is from [1412.6553 Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition](https://arxiv.org/abs/1412.6553)\n",
    "\n",
    "We can approximate our 4 dimensional convolutional kernel with a CP decomposition of rank R:\n",
    "\n",
    "$ K(i, j, s, t) = \\sum_{r=1}^R K^x_r(i)K^y_r(j)K^s_r(s)K^t_r(t) $.\n",
    "\n",
    "Plugging this into the formula for the convolutional layer output form above:\n",
    "\n",
    "$ V(x, y, t) = \\sum_r\\sum_i \\sum_j \\sum_sK^x_r(i)K^y_r(i)K^s_r(s)K^t_r(t)X(x-i, y-j, s) $\n",
    "$ = \\sum_rK^t_r(t) \\sum_i \\sum_j K^x_r(i)K^y_r(j)\\sum_sK^s_r(s)X(x-i, y-j, s) $ \n",
    "\n",
    "This gives us a recipe to do the convlution:\n",
    "\n",
    " 1. First do a point wise (1x1xS) convolution with the kernek $K(r)$.\n",
    " \n",
    " This reduces the number of input channels from S to R.\n",
    " The convolutions will next be done on a smaller number of channels, making them faster.\n",
    "\n",
    " 2. Perform seperable convolutions in the spatial dimensions with $K^x_r,K^y_r$. We can do this in PyTorch using grouped convolutions.\n",
    "\n",
    "    **Like in [mobilenets](https://arxiv.org/abs/1704.04861) the convolutions are depthwise seperable, done in each channel separately.**\n",
    "    **Unlike mobilenets the convolutions are also separable in the spatial dimensions.**\n",
    "\n",
    " 3. Do another pointwise convolution to change the number of channels from R to T\n",
    " 4. Finally, add the bias.\n",
    " \n",
    " \n",
    "Notice the combination of pointwise and depthwise convolutions like in mobilenets. While with mobilenets you have to train a network from scratch to get this structure, here we can decompose an existing layer into this form.\n",
    "\n",
    "As with mobile nets, to get the most speedup you will need a platform that has an efficient implementation of depthwise separable convolutions.\n",
    "\n",
    "Now we can write a function that receives a PyTorch Conv2D layer, and creates the CP decompisition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Before the decomposition', Conv2d (3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))\n",
      "('After the decomposition', Sequential(\n",
      "  (0): Conv2d (3, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): Conv2d (16, 16, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=16, bias=False)\n",
      "  (2): Conv2d (16, 16, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), groups=16, bias=False)\n",
      "  (3): Conv2d (16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using numpy backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorly.decomposition import parafac, partial_tucker\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def cp_decomposition_conv_layer(layer, rank):\n",
    "    \"\"\" Gets a conv layer and a target rank, \n",
    "        returns a nn.Sequential object with the decomposition \"\"\"\n",
    "\n",
    "    # Perform CP decomposition on the layer weight tensorly. \n",
    "    X = layer.weight.data.numpy()\n",
    "    last, first, vertical, horizontal = parafac(X, rank=rank, init = 'svd')\n",
    "\n",
    "    pointwise_s_to_r_layer = torch.nn.Conv2d(in_channels = first.shape[0], \\\n",
    "            out_channels = first.shape[1],\n",
    "            kernel_size = 1, \\\n",
    "            stride = 1,\n",
    "            padding = 0,\n",
    "            dilation = layer.dilation,\n",
    "            bias = False)\n",
    "\n",
    "    depthwise_vertical_layer = torch.nn.Conv2d(in_channels = vertical.shape[1], \\\n",
    "            out_channels = vertical.shape[1],\n",
    "            kernel_size = (vertical.shape[0], 1),\n",
    "            stride = 1,\n",
    "            padding = (layer.padding[0], 0),\n",
    "            dilation = layer.dilation,\n",
    "            groups = vertical.shape[1],\n",
    "            bias = False)\n",
    "\n",
    "    depthwise_horizontal_layer = torch.nn.Conv2d(in_channels = horizontal.shape[1], \\\n",
    "            out_channels = horizontal.shape[1],\n",
    "            kernel_size = (1, horizontal.shape[0]),\n",
    "            stride = layer.stride,\n",
    "            padding = (0, layer.padding[0]),\n",
    "            dilation = layer.dilation,\n",
    "            groups = horizontal.shape[1],\n",
    "            bias = False)\n",
    "\n",
    "    pointwise_r_to_t_layer = torch.nn.Conv2d(in_channels = last.shape[1], \\\n",
    "            out_channels = last.shape[0],\n",
    "            kernel_size = 1, \\\n",
    "            stride = 1,\n",
    "            padding = 0,\n",
    "            dilation = layer.dilation,\n",
    "            bias = True)\n",
    "    pointwise_r_to_t_layer.bias.data = layer.bias.data\n",
    "\n",
    "    # Transpose dimensions back to what PyTorch expects\n",
    "    depthwise_vertical_layer_weights = np.expand_dims(np.expand_dims(\\\n",
    "        vertical.transpose(1, 0), axis = 1), axis = -1)\n",
    "    depthwise_horizontal_layer_weights = np.expand_dims(np.expand_dims(\\\n",
    "        horizontal.transpose(1, 0), axis = 1), axis = 1)\n",
    "    pointwise_s_to_r_layer_weights = np.expand_dims(\\\n",
    "        np.expand_dims(first.transpose(1, 0), axis = -1), axis = -1)\n",
    "    pointwise_r_to_t_layer_weights = np.expand_dims(np.expand_dims(\\\n",
    "        last, axis = -1), axis = -1)\n",
    "\n",
    "    # Fill in the weights of the new layers\n",
    "    depthwise_horizontal_layer.weight.data = \\\n",
    "        torch.from_numpy(np.float32(depthwise_horizontal_layer_weights))\n",
    "    depthwise_vertical_layer.weight.data = \\\n",
    "        torch.from_numpy(np.float32(depthwise_vertical_layer_weights))\n",
    "    pointwise_s_to_r_layer.weight.data = \\\n",
    "        torch.from_numpy(np.float32(pointwise_s_to_r_layer_weights))\n",
    "    pointwise_r_to_t_layer.weight.data = \\\n",
    "        torch.from_numpy(np.float32(pointwise_r_to_t_layer_weights))\n",
    "\n",
    "    new_layers = [pointwise_s_to_r_layer, depthwise_vertical_layer, \\\n",
    "                    depthwise_horizontal_layer, pointwise_r_to_t_layer]\n",
    "    return nn.Sequential(*new_layers)\n",
    "\n",
    "print('Before the decomposition', layer)\n",
    "layer_cp_decomposed = cp_decomposition_conv_layer(layer, rank=16)\n",
    "print('After the decomposition', layer_cp_decomposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tucker decomposition on convolutional layers\n",
    "\n",
    "The idea here is from [*1511.06530 Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications*](https://arxiv.org/abs/1511.06530).\n",
    "\n",
    "Getting back to our kernel tensor K, we can approximate it using the Tucker decomposition:\n",
    "$ K(i, j, s, t) = \\sum_{r_1=1}^{R_1}\\sum_{r_2=1}^{R_2}\\sum_{r_3=1}^{R_3}\\sum_{r_4=1}^{R_4}\\sigma_{r_1 r_2 r_3 r_4} K^x_{r1}(i)K^y_{r2}(j)K^s_{r3}(s)K^t_{r4}(t) $\n",
    "\n",
    "The Tucker decomposition has the useful property that it doesn't have to be decomposed along all the axis (modes).\n",
    "\n",
    "Since the spatial dimensions are already quite small (3x3), it doesn't make a lot of sense to decompose those dimensions.\n",
    "\n",
    "We can perform the decomposition along the input and output channels instead (a mode-2 decomposition):\n",
    "\n",
    "$K(i, j, s, t) = \\sum_{r_3=1}^{R_3}\\sum_{r_4=1}^{R_4}\\sigma_{i j r_3 r_4}(j)K^s_{r3}(s)K^t_{r4}(t)$\n",
    "\n",
    "Like for CP decomposition, lets write the convolution formula and plug in the kernel decomposition: \n",
    "$ V(x, y, t) = \\sum_i \\sum_j \\sum_s\\sum_{r_3=1}^{R_3}\\sum_{r_4=1}^{R_4}\\sigma_{(i)(j) r_3 r_4}K^s_{r3}(s)K^t_{r4}(t)X(x-i, y-j, s) = \\sum_i \\sum_j \\sum_{r_4=1}^{R_4}\\sum_{r_3=1}^{R_3}K^t_{r4}(t)\\sigma_{(i)(j) r_3 r_4} \\sum_s\\ K^s_{r3}(s)X(x-i, y-j, s) $ \n",
    "\n",
    "This gives us the following recipe for doing the convolution with Tucker Decomposition:\n",
    "\n",
    " 1. Point wise convolution with $K^s_{r3}(s)$ for reducing the number of channels from S to $R_3$.\n",
    "\n",
    " 2. Regular (not separable) convolution with $\\sigma_{(i)(j) r_3 r_4} $.\n",
    " Instead of S input channels and T output channels like the original layer had,\n",
    " this convolution has $R_3$ input channels and $R_4$ output channels. If these ranks are smaller than S and T, this is were the speed gain comes from.\n",
    "\n",
    " 3. Pointwise convolution with $K^t_{r4}(t)$ to get back to T output channels like the original convolution.\n",
    " 4. Add the bias.\n",
    "\n",
    "Now we can write a function that receives a PyTorch Conv2D layer, and creates the tucker decompisition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Before the decomposition', Conv2d (3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))\n",
      "('After the decomposition', Sequential(\n",
      "  (0): Conv2d (3, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): Conv2d (3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (2): Conv2d (16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "def tucker_decomposition_conv_layer(layer, ranks):\n",
    "    \"\"\" Gets a conv layer, \n",
    "        returns a nn.Sequential object with the Tucker decomposition.\n",
    "    \"\"\"\n",
    "    \n",
    "    core, [last, first] = \\\n",
    "        partial_tucker(layer.weight.data.numpy(), \\\n",
    "            modes=[0, 1], ranks=ranks, init='svd')\n",
    "\n",
    "    # A pointwise convolution that reduces the channels from S to R3\n",
    "    first_layer = torch.nn.Conv2d(in_channels = first.shape[0], \\\n",
    "            out_channels = first.shape[1],\n",
    "            kernel_size = 1, \\\n",
    "            stride = 1,\n",
    "            padding = 0,\n",
    "            dilation = layer.dilation,\n",
    "            bias = False)\n",
    "\n",
    "    # A regular 2D convolution layer with R3 input channels \n",
    "    # and R3 output channels\n",
    "    core_layer = torch.nn.Conv2d(in_channels = core.shape[1], \\\n",
    "            out_channels = core.shape[0],\n",
    "            kernel_size = layer.kernel_size,\n",
    "            stride = layer.stride,\n",
    "            padding = layer.padding,\n",
    "            dilation = layer.dilation,\n",
    "            bias = False)\n",
    "\n",
    "    # A pointwise convolution that increases the channels from R4 to T\n",
    "    last_layer = torch.nn.Conv2d(in_channels = last.shape[1], \\\n",
    "            out_channels = last.shape[0],\n",
    "            kernel_size = 1, \\\n",
    "            stride = 1,\n",
    "            padding = 0,\n",
    "            dilation = layer.dilation,\n",
    "            bias = True)\n",
    "\n",
    "    last_layer.bias.data = layer.bias.data\n",
    "\n",
    "\n",
    "    # Transpose add dimensions to fit into the PyTorch tensors\n",
    "    first = first.transpose((1, 0))\n",
    "    first_layer.weight.data = torch.from_numpy(np.float32(\\\n",
    "        np.expand_dims(np.expand_dims(first.copy(), axis=-1), axis=-1)))\n",
    "    last_layer.weight.data = torch.from_numpy(np.float32(\\\n",
    "        np.expand_dims(np.expand_dims(last.copy(), axis=-1), axis=-1)))\n",
    "    core_layer.weight.data = torch.from_numpy(np.float32(core.copy()))\n",
    "\n",
    "    new_layers = [first_layer, core_layer, last_layer]\n",
    "    return nn.Sequential(*new_layers)\n",
    "\n",
    "print('Before the decomposition', layer)\n",
    "layer_tucker_decomposed = tucker_decomposition_conv_layer(layer, ranks=[16, 16])\n",
    "print('After the decomposition', layer_tucker_decomposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Being careful with strides and dilations\n",
    "\n",
    "Sometimes the convolutional layer will have a stride.\n",
    "In this case, the last convolution among the decomposed parts, should have the original layer's stride, and the rest should have just a stride of one.\n",
    "Since the last part of these decompositions is a 1x1 convolution, we can set the stride of the layer before it instead, to make it a bit faster. \n",
    "\n",
    "In case of dilations, all the spatial layers should preserve the dilation rate of the original layer.\n",
    "\n",
    "\n",
    "## Accelerating the entire network\n",
    "To accelerate the network, we can loop over the layers, and replace convolutional layers with their decomposition.\n",
    "We will count the number of parameters in the network before and after the decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d (3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace)\n",
      "  (2): Sequential(\n",
      "    (0): Conv2d (64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d (32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): Conv2d (32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (3): ReLU(inplace)\n",
      "  (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (5): Sequential(\n",
      "    (0): Conv2d (64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d (32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): Conv2d (64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (6): ReLU(inplace)\n",
      "  (7): Sequential(\n",
      "    (0): Conv2d (128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): Conv2d (64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (8): ReLU(inplace)\n",
      "  (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (10): Sequential(\n",
      "    (0): Conv2d (128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d (64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): Conv2d (128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (11): ReLU(inplace)\n",
      "  (12): Sequential(\n",
      "    (0): Conv2d (256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): Conv2d (128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (13): ReLU(inplace)\n",
      "  (14): Sequential(\n",
      "    (0): Conv2d (256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): Conv2d (128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (15): ReLU(inplace)\n",
      "  (16): Sequential(\n",
      "    (0): Conv2d (256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): Conv2d (128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (17): ReLU(inplace)\n",
      "  (18): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (19): Sequential(\n",
      "    (0): Conv2d (256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d (128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): Conv2d (256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (20): ReLU(inplace)\n",
      "  (21): Sequential(\n",
      "    (0): Conv2d (512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): Conv2d (256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (22): ReLU(inplace)\n",
      "  (23): Sequential(\n",
      "    (0): Conv2d (512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): Conv2d (256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (24): ReLU(inplace)\n",
      "  (25): Sequential(\n",
      "    (0): Conv2d (512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): Conv2d (256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (26): ReLU(inplace)\n",
      "  (27): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (28): Sequential(\n",
      "    (0): Conv2d (512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): Conv2d (256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (29): ReLU(inplace)\n",
      "  (30): Sequential(\n",
      "    (0): Conv2d (512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): Conv2d (256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (31): ReLU(inplace)\n",
      "  (32): Sequential(\n",
      "    (0): Conv2d (512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): Conv2d (256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (33): ReLU(inplace)\n",
      "  (34): Sequential(\n",
      "    (0): Conv2d (512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): Conv2d (256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (35): ReLU(inplace)\n",
      "  (36): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      ")\n",
      "('Number of parameters before the decomposition', 20024384L)\n",
      "('Number of parameters after the decomposition', 7278656L)\n",
      "('Ratio', 0.3634896334389113)\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce # Valid in Python 2.6+, required in Python 3\n",
    "import operator\n",
    "\n",
    "def count_params(model):    \n",
    "    return  sum(reduce(operator.mul, p.size(), 1) for p in model.parameters())\n",
    "\n",
    "params_before = count_params(model.features)\n",
    "\n",
    "for index, module in enumerate(model.features._modules):\n",
    "    if index > 0:\n",
    "        layer = model.features._modules[module]\n",
    "        if type(layer) is torch.nn.Conv2d:\n",
    "            ranks = [layer.weight.size(0)/2, layer.weight.size(1)/2]\n",
    "            model.features._modules[module] = tucker_decomposition_conv_layer(layer, ranks)\n",
    "        \n",
    "print model.features\n",
    "\n",
    "params_after = count_params(model.features)\n",
    "\n",
    "print('Number of parameters before the decomposition', params_before)\n",
    "print('Number of parameters after the decomposition', params_after)\n",
    "print('Ratio', float(params_after) / params_before)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
